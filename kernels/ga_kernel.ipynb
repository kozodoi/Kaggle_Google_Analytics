{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dddabb983cddc6cc669dfede6c3cb434a6d269cf"
   },
   "source": [
    "Data are generated from this script : https://www.kaggle.com/qnkhuat/make-data-ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from datetime import datetime\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "\n",
    "data_root = '../input/make-data-ready'\n",
    "print(os.listdir(data_root))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fed79ee0e6bd26d348d02b8c7b59886c3afe22c2"
   },
   "source": [
    "# Import and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor as RFF\n",
    "import xgboost as xgb\n",
    "\n",
    "from pprint import pprint\n",
    "import math\n",
    "\n",
    "from scipy.stats import kurtosis, skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "42d57caae8977328d06ce1191632cd93f93cceff"
   },
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "plt.rcParams['figure.figsize'] = (12,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c3cc9e31f9c51f1fd66593d2fd0cb3963e880f12"
   },
   "outputs": [],
   "source": [
    "def load_data(data='train',n=2):\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(n) :\n",
    "        if data=='train':\n",
    "            if i > 8 :\n",
    "                break\n",
    "            dfpart = pd.read_pickle(pjoin(data_root,f'train_{i}.pkl'))\n",
    "        elif data=='test':\n",
    "            if i > 2 :\n",
    "                break\n",
    "            dfpart = pd.read_pickle(pjoin(data_root,f'test_{i}.pkl'))\n",
    "        df = pd.concat([df,dfpart])\n",
    "        del dfpart\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "711b2ff1d23636474cbf8cbd63ed852d4853499d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_train = load_data(n=9)\n",
    "df_test = load_data('test',n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "660a0bb0b6ac94d469e290bddbf0b32ca55bf0ff",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'# of columns has na value: {(df_test.isnull().sum().sort_values(ascending=False) > 0).sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3609ea5ecb4ea0a721136c3fc804b48877df40fd"
   },
   "source": [
    "# Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6c8fa14fd59dfe63ee72f2b5f8f1f8768894170e"
   },
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return round(np.sqrt(mean_squared_error(y_true, y_pred)), 5)\n",
    "\n",
    "def split_data(df=df_train,rate=.8):\n",
    "    # sort the date first\n",
    "    df = df.sort_values('date').copy()\n",
    "    \n",
    "    df.drop(['fullVisitorId','visitId','visitStartTime'],axis=1,inplace=True)\n",
    "    df['Revenue'] = np.log1p(df['Revenue'])\n",
    "    \n",
    "    global X_train,X_valid,y_train,y_valid\n",
    "    \n",
    "    n_train = int(len(df)*rate)\n",
    "    X_train = df.drop(['Revenue','date'],axis=1).iloc[:n_train]\n",
    "    X_valid = df.drop(['Revenue','date'],axis=1).iloc[n_train:]\n",
    "    \n",
    "    y_train = df['Revenue'].iloc[:n_train]\n",
    "    y_valid = df['Revenue'].iloc[n_train:]\n",
    "    \n",
    "    print(X_train.shape,X_valid.shape)\n",
    "    \n",
    "    \n",
    "\n",
    "def encode_data(verbose=False):\n",
    "    global df_train_encoded,df_test_encoded\n",
    "    df_train_encoded = df_train.copy()\n",
    "    df_test_encoded = df_test.copy()\n",
    "    for col in df_train.columns:\n",
    "        if df_train_encoded[col].dtype == 'object' and col not in ['fullVisitorId','visitId','visitStartTime','date']:\n",
    "            if verbose:\n",
    "                print(col)\n",
    "            lb = LabelEncoder()\n",
    "            lb.fit( list(df_train_encoded[col].unique()) + list(df_test_encoded[col].unique()))\n",
    "            df_train_encoded[col] = lb.transform(df_train_encoded[col])\n",
    "            df_test_encoded[col] = lb.transform(df_test_encoded[col])\n",
    "        \n",
    "def run_xgb():\n",
    "   \n",
    "    params = {\n",
    "        'objective':'reg:linear',\n",
    "        'eval_metric':'rmse',\n",
    "        'learning_rate':.01,\n",
    "        'eta': 0.15, # Step size shrinkage used in update to prevents overfitting\n",
    "#         'max_depth': 10, # V3 : 1.0471 on LB\n",
    "#         'max_depth':5, # V5 : 0.9331 on LB\n",
    "        'subsample': 0.6, # sample of rows\n",
    "        'colsample_bytree': 0.6, # sample of features\n",
    "#         'alpha':0.001, \n",
    "        'lambda':1, # l2 regu\n",
    "        'random_state': 42,\n",
    "        'silent':True\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    # got params from https://www.kaggle.com/kailex/group-xgb-for-gstore-v2\n",
    "    params['n_thread'] = -1\n",
    "    params['max_depth'] = 8\n",
    "    params['min_child_weight'] = 100\n",
    "    params['gamma'] = 5\n",
    "    params['subsample'] = 1\n",
    "    params['colsample_bytree'] = .95\n",
    "    params['colsample_bylevel'] = 0.35\n",
    "    params['alpha'] = 25\n",
    "    params['lambda'] = 25\n",
    "    \n",
    "    xgb_train_data = xgb.DMatrix(X_train, y_train)\n",
    "    xgb_val_data = xgb.DMatrix(X_valid, y_valid)\n",
    "    \n",
    "    model = xgb.train(params, xgb_train_data,\n",
    "#           num_boost_round=1000, # V3 : 1.0471 on LB\n",
    "#           num_boost_round=200, # 1.0471 on LB\n",
    "          num_boost_round = 200,\n",
    "          evals= [(xgb_train_data, 'train'), (xgb_val_data, 'valid')],\n",
    "#           early_stopping_rounds=10, # V11 0.9301 on LB\n",
    "          early_stopping_rounds=50, \n",
    "          verbose_eval=20\n",
    "         )\n",
    "    return model\n",
    "\n",
    "def submit():\n",
    "    test_matrix = xgb.DMatrix(X_test)\n",
    "    y_pred = clf.predict(test_matrix,ntree_limit=clf.best_ntree_limit)\n",
    "    df_test['PredictedLogRevenue'] = y_pred\n",
    "    engineer_prediction\n",
    "    print('rmse after engineer prediction')\n",
    "    print(rmse(y_pred,df_test['PredictedLogRevenue']))\n",
    "    submit = df_test[['PredictedLogRevenue','fullVisitorId']].groupby('fullVisitorId').PredictedLogRevenue.sum().reset_index()\n",
    "    submit.to_csv('submit.csv',index=False)\n",
    "    \n",
    "    test(y_pred)\n",
    "    \n",
    "    \n",
    "def engineer_prediction(df_test):\n",
    "    df_test[df_test['totals_hits'] == 1].PredictedLogRevenue = 0\n",
    "    df_test[df_test['totals_timeOnSite'] == 0].PredictedLogRevenue = 0\n",
    "    df_test[df_test['totals_bouces'] == 1].PredictedLogRevenue = 0\n",
    "    return dftest\n",
    "    \n",
    "    \n",
    "def test(predict):\n",
    "    y_test = np.log1p(df_test['totals_transactionRevenue'])\n",
    "    print(rmse(y_test,predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c803f389fc7a88ef267e7ed031e0b0649a910ce8"
   },
   "outputs": [],
   "source": [
    "def prepare_data(df_train,df_test,\n",
    "                 del_col=['fullVisitorId','visitId','visitStartTime','date'],to_log=None):\n",
    "    df_train = df_train.sort_values('date').copy()\n",
    "    \n",
    "    df_train = df_train.drop(del_col,axis=1).copy()\n",
    "    df_test = df_test.drop(del_col,axis=1).copy()\n",
    "    \n",
    "    # Log some column\n",
    "    if to_log is not None:\n",
    "        df_train[to_log] = np.log1p(df_train[to_log])\n",
    "        df_test[to_log] = np.log1p(df_test[to_log])\n",
    "    \n",
    "    # totals_transactionRevenue\n",
    "    df_train['totals_transactionRevenue'] = np.log1p(df_train['totals_transactionRevenue'])\n",
    "    df_test['totals_transactionRevenue'] = np.log1p(df_test['totals_transactionRevenue'])\n",
    "    \n",
    "    global X_train,X_valid,y_train,y_valid,X_test,y_test\n",
    "    # 80/20 : train/valid\n",
    "    n_train = int(len(df_train)*.8)\n",
    "    \n",
    "    # split\n",
    "    X_train = df_train.drop(['totals_transactionRevenue'],axis=1).iloc[:n_train]\n",
    "    X_valid = df_train.drop(['totals_transactionRevenue'],axis=1).iloc[n_train:]\n",
    "    \n",
    "    y_train = df_train['totals_transactionRevenue'].iloc[:n_train]\n",
    "    y_valid = df_train['totals_transactionRevenue'].iloc[n_train:]\n",
    "    \n",
    "    X_test = df_test.drop(['totals_transactionRevenue'],axis=1)\n",
    "    y_test = df_test['totals_transactionRevenue']\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "66568ffac07e4667abb96a0dd46bd640477ff086"
   },
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    df = df.copy()\n",
    "    # Copy from : https://www.kaggle.com/qnkhuat/base-model-v2-with-with-full-features/edit\n",
    "    \n",
    "    # time based\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['weekday'] = df['date'].dt.weekday\n",
    "    df['weekofyear'] = df['date'].dt.weekofyear\n",
    "    \n",
    "    df['browser_category'] = df['device_browser'] + '_' + df['device_deviceCategory']\n",
    "    df['browser_operatingSystem'] = df['device_browser'] + '_' + df['device_operatingSystem']\n",
    "\n",
    "    df['month_unique_user_count'] = df.groupby('month')['fullVisitorId'].transform('nunique')\n",
    "    df['day_unique_user_count'] = df.groupby('day')['fullVisitorId'].transform('nunique')\n",
    "    df['weekday_unique_user_count'] = df.groupby('weekday')['fullVisitorId'].transform('nunique')\n",
    "    df['weekofyear_unique_user_count'] = df.groupby('weekofyear')['fullVisitorId'].transform('nunique')\n",
    "    \n",
    "    \n",
    "    df['sum_pageviews_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_pageviews'].transform('sum')\n",
    "    df['count_pageviews_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_pageviews'].transform('count')\n",
    "    df['mean_pageviews_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_pageviews'].transform('mean')\n",
    "    df['sum_hits_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_hits'].transform('sum')\n",
    "    df['count_hits_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_hits'].transform('count')\n",
    "    df['mean_hits_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_hits'].transform('mean')\n",
    "    \n",
    "    df['mean_hits_per_day'] = df.groupby(['day'])['totals_hits'].transform('mean')\n",
    "    df['sum_hits_per_day'] = df.groupby(['day'])['totals_hits'].transform('sum')\n",
    "    \n",
    "    df['sum_pageviews_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_pageviews'].transform('sum')\n",
    "    df['count_pageviews_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_pageviews'].transform('count')\n",
    "    df['mean_pageviews_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_pageviews'].transform('mean')\n",
    "\n",
    "    df['sum_pageviews_per_region'] = df.groupby('geoNetwork_region')['totals_pageviews'].transform('sum')\n",
    "    df['count_pageviews_per_region'] = df.groupby('geoNetwork_region')['totals_pageviews'].transform('count')\n",
    "    df['mean_pageviews_per_region'] = df.groupby('geoNetwork_region')['totals_pageviews'].transform('mean')\n",
    "    \n",
    "    df['sum_hits_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_hits'].transform('sum')\n",
    "    df['count_hits_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_hits'].transform('count')\n",
    "    df['mean_hits_per_network_domain'] = df.groupby('geoNetwork_networkDomain')['totals_hits'].transform('mean')\n",
    "\n",
    "    df['sum_hits_per_region'] = df.groupby('geoNetwork_region')['totals_hits'].transform('sum')\n",
    "    df['count_hits_per_region'] = df.groupby('geoNetwork_region')['totals_hits'].transform('count')\n",
    "    df['mean_hits_per_region'] = df.groupby('geoNetwork_region')['totals_hits'].transform('mean')\n",
    "\n",
    "    df['sum_hits_per_country'] = df.groupby('geoNetwork_country')['totals_hits'].transform('sum')\n",
    "    df['count_hits_per_country'] = df.groupby('geoNetwork_country')['totals_hits'].transform('count')\n",
    "    df['mean_hits_per_country'] = df.groupby('geoNetwork_country')['totals_hits'].transform('mean')\n",
    "    \n",
    "    df['user_pageviews_sum'] = df.groupby('fullVisitorId')['totals_pageviews'].transform('sum')\n",
    "    df['user_hits_sum'] = df.groupby('fullVisitorId')['totals_hits'].transform('sum')\n",
    "    \n",
    "    df['user_pageviews_count'] = df.groupby('fullVisitorId')['totals_pageviews'].transform('count')\n",
    "    df['user_hits_count'] = df.groupby('fullVisitorId')['totals_hits'].transform('count')\n",
    "\n",
    "    \n",
    "    df['user_pageviews_sum_to_mean'] = df['user_pageviews_sum'] / df['user_pageviews_sum'].mean()\n",
    "    df['user_hits_sum_to_mean'] = df['user_hits_sum'] / df['user_hits_sum'].mean()\n",
    "\n",
    "    df['user_pageviews_to_region'] = df['user_pageviews_sum'] / df['mean_pageviews_per_region']\n",
    "    df['user_hits_to_region'] = df['user_hits_sum'] / df['mean_hits_per_region']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0013141da2777f0ac9d4b2feffed3c153930c44e"
   },
   "outputs": [],
   "source": [
    "df_train = feature_engineering(df_train)\n",
    "df_test = feature_engineering(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "16605dd219a0e49bb3ab9ddd0629c34ea7f948e1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encode_data(verbose=True)\n",
    "prepare_data(df_train_encoded,df_test_encoded,del_col=['fullVisitorId','visitId',\n",
    "            'visitStartTime','date'],to_log=['visitNumber','totals_hits','totals_pageviews',\n",
    "            'totals_sessionQualityDim','totals_timeOnSite','trafficSource_keyword'])\n",
    "# prepare_data(df_train_encoded,df_test_encoded,del_col=['fullVisitorId','visitId',\n",
    "#             'visitStartTime','date'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c26b1e30bf504d9c9cec95146656358ed844970e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf = run_xgb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b3094980c0ec926539821a462d0ee2d65abe9ea"
   },
   "outputs": [],
   "source": [
    "# try to find a good validation set\n",
    "# Why our score so different with the leader board?\n",
    "# check with the target in dataset first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "392938eb9284e3f800c7f1254891827b39c68d7d"
   },
   "source": [
    "# Feature important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0101c08148886885c07d41dfa152fb94fc8e604d"
   },
   "source": [
    "\"gain\" is the average gain of splits which use the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "63cb30a84b68bec37fe1ffa53d6c708b489731fd"
   },
   "outputs": [],
   "source": [
    "xgb.plot_importance(clf,importance_type='gain',max_num_features=20)\n",
    "plt.title('Gain Feature important')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "81d96c5a110b876e555d19e538fc888d0b051001"
   },
   "source": [
    "\"cover\" is the average coverage of splits which use the feature where coverage is defined as the number of samples affected by the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "34b2a8c2186d69867ae376671ff291fce662fc99",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xgb.plot_importance(clf,importance_type='cover',max_num_features=20)\n",
    "plt.title('Cover Feature important')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22b8e759c46de5a8031ad3512f7d0b7e679b0a84"
   },
   "source": [
    "\"weight\" is the number of times a feature appears in a tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e32fac1da52c2a2e7644e1af7c9a196f573f59dc",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xgb.plot_importance(clf,importance_type='weight',max_num_features=20)\n",
    "plt.title('Weight Feature important')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5dc30a7c4a402a664bcade5046b382ec160a9268"
   },
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a5c973b9c7ef1d533fcb4c807b7d39a123dfd876"
   },
   "outputs": [],
   "source": [
    "submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9f587453cb66d50c15822707425190512ddab4d0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5b4c795987399a376ff7f856b2a43d62e76ef57e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "171b4868f331037343e0930111a72edb4c29860a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e32583cc487663dd61799cb1960f872eff20bd2c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d48f9ea4532b68da11401a92d6dc020f33c791a7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "24e84945849d90581707fd71c587ead4ba709583"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
